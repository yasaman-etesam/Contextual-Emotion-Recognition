<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Visual Emotional Theory of Mind: Large Vision and Language Models' Capabilities and Limitations">
    <meta name="author" content="Yasaman Etesam,
                                 Ozge Nilay Yalcin,
                                 Chuxuan Zhang,
                                  Angelica Lim">

    <title>Visual Emotional Theory of Mind: Large Vision and Language Models' Capabilities and Limitations</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">

    <script type="module"
            src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
    </script>
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Visual Emotional Theory of Mind: Large Vision and Language Models' Capabilities and Limitations</h2>
    <!-- <h2>CRV 2022</h2> -->
    <hr>
    <p class="authors">
        <a href="https://www.linkedin.com/in/yasaman-etesam/"> Yasaman Etesam</a>,
        <a href="https://www.linkedin.com/in/ozge-nilay-yalcin/"> Ozge Nilay Yalcin</a>, 
	<a href="http://www.linkedin.com/in/chuxuan-zhang7"> Chuxuan Zhang</a>,
        <a href="https://www.linkedin.com/in/angelicajeannelim/"> Angelica Lim</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/rosielab/Fast-and-Slow">Code</a>
        <a class="btn btn-primary" href="http://arxiv.org/abs/2310.19995">Paper</a> 
    </div>
</div>

<div class="container">
    <div class="section">
	    <h2>Overview</h2>
        <p>
The visual emotional theory of mind problem is an image emotion recognition task, specifically asking "How does the person in the bounding box feel?" Facial expressions, body pose, contextual understanding and implicit commonsense knowledge all contribute to the difficulty of the task, making this type of emotion estimation in context currently one of the hardest problems in affective computing. The goal of this work is to evaluate the emotional knowledge embedded in recent large vision language models (CLIP, LLaVA) and large language models (GPT-3.5, MPT, Falcon, Baichuan) on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely text-based language model on images, we construct "narrative captions" relevant to emotion perception, using a set of 872 physical social signal descriptions related to 26 emotional categories, along with 224 labels for emotionally salient environmental contexts, sourced from writer's guides for character expressions and settings. We evaluate the use of the resulting captions in an image-to-language-to-emotion task. Experiments using zero-shot vision-language models on EMOTIC show that a considerable gap remains in the emotional theory of mind task compared to prior work trained on the dataset, and that captioning with physical signals and environment provides a better basis for emotion recognition than captions based only on activity. Also, based on our findings, we propose that combining "fast" and "slow" reasoning is a promising way forward to improve emotion recognition systems.
	</p>

<div class="section">
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
		<img src="figs/example.png" style="width:50%; margin-right:-20px; margin-top:-10px;">
            </div>
	
        </div>
	<p>
            An example output comparing our method to <a href="https://github.com/haotian-liu/LLaVA"> LLaVA</a> and CLIP.
        </p>
        <hr>        
    </div>


        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
					<img src="figs/results.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
            </div>
        </div>
        <p>
            Results of different methods for 26 different categories from <a href="https://s3.sunai.uoc.edu/emotic/index.html"> EMOTIC</a>
        </p>
	 <hr>

    </div>


    <div class="section">
        <h2>Download</h2>
        <hr>
        <div>
            <div class="download files">
                <p>Below one can download the generated captions. </p>
                coming soon!
            </div>
        </div>
    </div>

    
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
<!--         <div class="bibtexsection">
         @inproceedings{,
            author = {},
            title = {Visual Emotional Theory of Mind: Large Vision and Language Models' Capabilities and Limitations},
            booktitle = {},
            year = 2023
         }            		
        </div> -->
    </div>

<!--     <div class="section">
        <h2>Acknowledgements</h2>
        <hr>
        <div class="acknowledgements">
            We thank NSERC Discovery Grant
        </div>
    </div> -->

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="mailto: yetesam@sfu.ca">Yasaman Etesam</a>. Website template from <a href="https://www.vincentsitzmann.com/metasdf/">MetaSDF</a>. </p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
