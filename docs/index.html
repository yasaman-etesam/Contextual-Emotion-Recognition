<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Contextual Emotion Recognition using Large Vision Language Models & Emotional Theory of Mind">
    <meta name="author" content="Yasaman Etesam,
                                 Ozge Nilay Yalcin,
                                 Chuxuan Zhang,
                                  Angelica Lim">

    <title>Contextual Emotion Recognition using Large Vision Language Models</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">

    <script type="module"
            src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
    </script>
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Contextual Emotion Recognition using Large Vision Language Models</h2>
    <!-- <h2>CRV 2022</h2> -->
    <hr>
    <p class="authors">
        <a href="https://www.linkedin.com/in/yasaman-etesam/"> Yasaman Etesam</a>,
        <a href="https://www.linkedin.com/in/ozge-nilay-yalcin/"> Ozge Nilay Yalcin</a>, 
	<a href="http://www.linkedin.com/in/chuxuan-zhang7"> Chuxuan Zhang</a>,
        <a href="https://www.linkedin.com/in/angelicajeannelim/"> Angelica Lim</a>
    </p>
    <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://github.com/yasaman-etesam/Contextual-Emotion-Recognition">Code</a>
	<a class="btn btn-primary" href="https://huggingface.co/yetesam/LLaVA-Finetuned-Contextual-Emotion-Recognition">Weights</a>
        <a class="btn btn-primary" href="https://arxiv.org/pdf/2405.08992">Paper1</a> 
	<a class="btn btn-primary" href="https://arxiv.org/pdf/2310.19995">Paper2</a>
    </div>
</div>

<div class="container">
    <div class="section">
	    <h2>Overview</h2>
        <p>
"How does the person in the bounding box feel?" Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a fine-tuned vision language model, even on a small dataset, significantly outperforms traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future.	</p>

<div class="section">
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
		<img src="figs/example.png" style="width:50%; margin-right:-20px; margin-top:-10px;">
            </div>
	
        </div>
	<p>
            An example output showing results of different methods.
        </p>
        <hr>        
    </div>


        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
					<img src="figs/results.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
            </div>
        </div>
        <p>
            Results of different methods for 26 different categories from <a href="https://s3.sunai.uoc.edu/emotic/index.html"> EMOTIC</a>
        </p>
	 <hr>

    </div>


    <div class="section">
        <h2>Captions</h2>
        <hr>
        <div>
            <div class="download files">
                <p> Download NarraCap for EMOTIC test set <a href="https://drive.google.com/file/d/1Fx84KysUSFFyBCBgFTe8bPUz_mCR1dnr/view?usp=share_link"> here</a>. </p>
		<p> Download NarraCap for EMOTIC val set <a href="https://drive.google.com/file/d/1UU8BmGDmbR63nYMIlHIUnnFJv9he0oRQ/view?usp=sharing"> here</a>. </p>
		<p> Download NarraCapXL for EMOTIC test set <a href="https://drive.google.com/file/d/1VyLOhqg025bYTN_E_cUH43t90jxc2i_j/view?usp=share_link"> here</a>. </p>
	    </div>
        </div>
    </div>

    
    <div class="section">
        <h2>Bibtex</h2>
        <hr>
<!--         <div class="bibtexsection">
         @inproceedings{,
            author = {},
            title = {Contextual Emotion Recognition using Large Vision Language Models},
            booktitle = {},
            year = 2023
         }            		
        </div> -->
    </div>

<!--     <div class="section">
        <h2>Acknowledgements</h2>
        <hr>
        <div class="acknowledgements">
            We thank NSERC Discovery Grant
        </div>
    </div> -->

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="mailto: yetesam@sfu.ca">Yasaman Etesam</a>. Website template from <a href="https://www.vincentsitzmann.com/metasdf/">MetaSDF</a>. </p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
